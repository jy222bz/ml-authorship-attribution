{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8um2yEHWUf2x"
   },
   "source": [
    "# Text Segmments Processor\n",
    " *  Jacob Yousif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdnfmL1xWWjY"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import textstat\n",
    "import textblob\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n",
    "from textstat.textstat import textstatistics\n",
    "import collections as coll\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'datasets/SegmentedLiterature.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has NaN values: False\n"
     ]
    }
   ],
   "source": [
    "has_nan = df.isna().any().any()\n",
    "print(f\"DataFrame has NaN values: {has_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents(text):\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_reading(text):\n",
    "    return textstatistics().flesch_reading_ease(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_level(text):\n",
    "    return textstatistics().flesch_kincaid_grade(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunning_fog(text):\n",
    "    return textstatistics().gunning_fog(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "    words = tokenize_words(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length_by_word(text):\n",
    "    sentences = tokenize_sents(text)\n",
    "    words = tokenize_words(text)\n",
    "    return len(words) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_syllables_per_word(text):\n",
    "    words = tokenize_words(text)\n",
    "    return textstat.syllable_count(text) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(text):\n",
    "    chars = set(\",.'!\\\";?:\")\n",
    "    return sum(char in chars for char in text) / max(1, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_functional_words(text):\n",
    "    words = tokenize_words(text)\n",
    "    functional_words = nltk.corpus.stopwords.words('english')\n",
    "    num_functional_words = sum(1 for word in words if word.lower() in functional_words)\n",
    "    return num_functional_words / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dale_chall_readability(text):\n",
    "    return textstat.dale_chall_readability_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpsons_index(text):\n",
    "    words = tokenize_words(text)\n",
    "    word_freq = Counter(words)\n",
    "    return 1 - sum((freq / len(words)) ** 2 for freq in word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(text):\n",
    "    words = tokenize_words(text)\n",
    "    word_freq = Counter(words)\n",
    "    probs = [freq / len(words) for freq in word_freq.values()]\n",
    "    return scipy.stats.entropy(probs, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yules_characteristic_k(text):\n",
    "    words = tokenize_words(text)\n",
    "    N = len(words)\n",
    "    word_freq = Counter(words)\n",
    "    M1 = N\n",
    "    M2 = sum(freq * (freq - 1) for freq in word_freq.values())\n",
    "    return 10**4 * ((M2 / (M1**2)) + (1 / M1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brunets_measure_w(text):\n",
    "    words = tokenize_words(text)\n",
    "    V = len(set(words))\n",
    "    N = len(words)\n",
    "    return N * (V ** -0.172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(text):\n",
    "    words = tokenize_words(text)\n",
    "    V = len(set(words))\n",
    "    N = len(words)\n",
    "    return V / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapax_dis_legomena(text):\n",
    "    words = tokenize_words(text)\n",
    "    word_freq = Counter(words)\n",
    "    h = sum(1 for _, freq in word_freq.items() if freq == 2)\n",
    "    hapax_legomena_count = len([word for word, count in word_freq.items() if count == 1])\n",
    "    \n",
    "    if hapax_legomena_count == 0:\n",
    "        s = 0\n",
    "    else:\n",
    "        s = 2 * h / hapax_legomena_count\n",
    "    \n",
    "    return s, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapax_legomena(text):\n",
    "    words = tokenize_words(text)\n",
    "    N = len(words)\n",
    "    \n",
    "    if N == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    h_count = len([word for word, count in Counter(words).items() if count == 1])\n",
    "    h = h_count / N\n",
    "    \n",
    "    if h == 1:\n",
    "        honore_r = float('inf') \n",
    "    else:\n",
    "        honore_r = (100 * math.log(N)) / (1 - h)\n",
    "    \n",
    "    return honore_r, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Book\", \"Text\", \"Author\", \"AuthorCode\", \"AverageWordLength\", \"AverageSentenceLength\", \"AverageSyllablePerWord\",\n",
    "    \"PunctuationCount\", \"FunctionalWordsCount\", \"TypeTokenRatio\", \"HonoreMeasureR\",\n",
    "    \"Hapax\", \"SichelesMeasureS\", \"Dihapax\", \"YulesCharacteristicK\", \"SimpsonsIndex\",\n",
    "    \"BrunetsMeasureW\", \"ShannonEntropy\", \"FleschReadingEase\", \"FleschKincaidGradeLevel\",\n",
    "    \"DaleChallReadability\", \"GunningFog\"\n",
    "]\n",
    "\n",
    "result = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text = row['Text']\n",
    "    honore_measure_r, hapax = hapax_legomena(text)\n",
    "    sicheles_measure_s, dihapax = hapax_dis_legomena(text)\n",
    "    _entry = {\n",
    "        \"Book\": row['Book'],\n",
    "        \"Text\": row['Text'],\n",
    "        \"Author\": row['Author'],\n",
    "        \"AuthorCode\": row['AuthorCode'],\n",
    "        \"AverageWordLength\": average_word_length(text),\n",
    "        \"AverageSentenceLength\": average_sentence_length_by_word(text),\n",
    "        \"AverageSyllablePerWord\": average_syllables_per_word(text),\n",
    "        \"PunctuationCount\": count_punctuation(text),\n",
    "        \"FunctionalWordsCount\": count_functional_words(text),\n",
    "        \"TypeTokenRatio\": type_token_ratio(text),\n",
    "        \"HonoreMeasureR\": honore_measure_r,\n",
    "        \"Hapax\": hapax,\n",
    "        \"SichelesMeasureS\": sicheles_measure_s,\n",
    "        \"Dihapax\": dihapax,\n",
    "        \"YulesCharacteristicK\": yules_characteristic_k(text),\n",
    "        \"SimpsonsIndex\": simpsons_index(text),\n",
    "        \"BrunetsMeasureW\": brunets_measure_w(text),\n",
    "        \"ShannonEntropy\": shannon_entropy(text),\n",
    "        \"FleschReadingEase\": flesch_reading(text),\n",
    "        \"FleschKincaidGradeLevel\": grade_level(text),\n",
    "        \"DaleChallReadability\": dale_chall_readability(text),\n",
    "        \"GunningFog\": gunning_fog(text),\n",
    "    }\n",
    "    _row = pd.DataFrame([_entry])\n",
    "    result = pd.concat([result, _row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 106792 rows\n"
     ]
    }
   ],
   "source": [
    "print('The size of the dataset is:', len(result), 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has NaN values: False\n"
     ]
    }
   ],
   "source": [
    "has_nan = result.isna().any().any()\n",
    "print(f\"DataFrame has NaN values: {has_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has NaN values: False\n"
     ]
    }
   ],
   "source": [
    "has_nan = result.isna().any().any()\n",
    "print(f\"DataFrame has NaN values: {has_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 106792 rows\n"
     ]
    }
   ],
   "source": [
    "print('The size of the dataset is:', len(result), 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'Datasets/ProcessedSegmentedLiterature.csv'\n",
    "result.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
